{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c51687e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "#1.Remove भोकणे and सुरुवात"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bfcfb7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9836 entries\n",
      "\n",
      "Processed 9836 entries\n",
      "Duplicate entry_ids: 0\n",
      "✅ All entry_ids are unique\n",
      "\n",
      "Example entry (index 300):\n",
      "{\n",
      "  \"entry_id\": \"berntsen_अवलंबून_5\",\n",
      "  \"headword_devanagari\": \"अवलंबून\",\n",
      "  \"headword_romanized\": \"avalambūna\",\n",
      "  \"full_entry\": \"अवलंबून avalambūna , असणे asaṇē to be dependent on.\",\n",
      "  \"source_page\": 5,\n",
      "  \"entry_type\": \"headword\",\n",
      "  \"base_word\": null,\n",
      "  \"search_text\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 1: Import and Schema Setup (FIXED - handles duplicate headwords)\n",
    "# ==============================================================================\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "# Load the dictionary\n",
    "with open('berntsen_dictionary.json', 'r', encoding='utf-8') as f:\n",
    "    original_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(original_data)} entries\")\n",
    "\n",
    "# Track seen base_ids to handle duplicates\n",
    "seen_base_ids = {}\n",
    "\n",
    "# Create new data with proper field ordering and complete schema\n",
    "data = []\n",
    "for entry in original_data:\n",
    "    new_entry = OrderedDict()\n",
    "    \n",
    "    # Generate base_id from headword + page\n",
    "    base_id = f\"berntsen_{entry['headword_devanagari']}_{entry['source_page']}\"\n",
    "    \n",
    "    # Handle duplicates by adding sequence number\n",
    "    if base_id in seen_base_ids:\n",
    "        seen_base_ids[base_id] += 1\n",
    "        entry_id = f\"{base_id}_{seen_base_ids[base_id]}\"\n",
    "    else:\n",
    "        seen_base_ids[base_id] = 1\n",
    "        entry_id = base_id\n",
    "    \n",
    "    # Core identification fields\n",
    "    new_entry['entry_id'] = entry_id\n",
    "    new_entry['headword_devanagari'] = entry['headword_devanagari']\n",
    "    new_entry['headword_romanized'] = entry['headword_romanized']\n",
    "    new_entry['full_entry'] = entry['full_entry']\n",
    "    new_entry['source_page'] = entry['source_page']\n",
    "    new_entry['entry_type'] = 'headword'\n",
    "    new_entry['base_word'] = None\n",
    "    new_entry['search_text'] = None\n",
    "    \n",
    "    data.append(new_entry)\n",
    "\n",
    "# Verify no duplicates\n",
    "entry_ids = [e['entry_id'] for e in data]\n",
    "duplicates = len(entry_ids) - len(set(entry_ids))\n",
    "print(f\"\\nProcessed {len(data)} entries\")\n",
    "print(f\"Duplicate entry_ids: {duplicates}\")\n",
    "if duplicates == 0:\n",
    "    print(\"✅ All entry_ids are unique\")\n",
    "\n",
    "print(\"\\nExample entry (index 300):\")\n",
    "print(json.dumps(data[300], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a20484bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Berntsen Dictionary, how part of speech is represented\n",
    "# Pattern 1: Sequential POS sections (most common)\n",
    "#       form: headword romanized POS1 definition(s). POS2 definition(s).\n",
    "#       example: \"अति ati adv. too much. pref. extremely, too much, over-.\"\n",
    "#       description: Each POS gets its own definition section which is separated by periods\n",
    "# Pattern 2: Numbered definitions within same POS\n",
    "#       form: headword romanized POS 1. def1. 2. def2. 3. def3.\n",
    "#       example: \"अंक aṅka m. 1. number. 2. issue (of a magazine, newspaper). 3. act (of a play).\"\n",
    "#       description: Numbers belong to the SAME POS. All numbered definitions share the first POS mentioned\n",
    "# Pattern 3: Mixed - numbered defs THEN new POS\n",
    "#       form: headword romanized POS1 1. def1. 2. def2. POS2 def3.\n",
    "#       example: \"अखंड akhaṇḍa adj. inv. 1. entire, in one piece. 2. continuous. adv. continuously.\"\n",
    "#       explanation of example: Definitions 1 and 2 belong to adj. inv. \n",
    "#       When adv. appears, it starts a NEW section with its own definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "277461a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries: 9836\n",
      "Entries with definitions: 9836 (100.0%)\n",
      "Entries without definitions: 0 (0.0%)\n",
      "Entries with null POS: 348 (3.5%)\n",
      "Total definitions parsed: 12167\n",
      "Average definitions per entry: 1.24\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2: Parse Definitions\n",
    "# ==============================================================================\n",
    "# This cell:\n",
    "# - Parses full_entry to extract individual definitions\n",
    "# - Identifies POS markers and numbered definitions\n",
    "# - Creates definitions[] array with full schema:\n",
    "#   definition, pos_display, number, pos, gender, declension_class, referenced_entry\n",
    "# - referenced_entry is initialized as None, populated in later cells\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_definitions(entry):\n",
    "    \"\"\"\n",
    "    Parse the full_entry to extract definitions with their POS and numbers.\n",
    "    \n",
    "    Args:\n",
    "        entry: Dictionary containing 'full_entry', 'headword_romanized'\n",
    "    \n",
    "    Returns:\n",
    "        List of definition dictionaries with full schema\n",
    "    \"\"\"\n",
    "    full_entry = entry['full_entry']\n",
    "    romanized = entry['headword_romanized']\n",
    "    \n",
    "    # Step 1: Extract the definition part (everything after romanized headword)\n",
    "    parts = full_entry.split(romanized, 1)\n",
    "    if len(parts) <= 1:\n",
    "        return []\n",
    "    \n",
    "    definition_text = parts[1].strip()\n",
    "    \n",
    "    # If definition_text starts with comma, remove it\n",
    "    if definition_text.startswith(','):\n",
    "        definition_text = definition_text[1:].strip()\n",
    "    \n",
    "    if not definition_text:\n",
    "        return []\n",
    "    \n",
    "    # Step 2: Define all possible POS markers (order matters - longest first)\n",
    "    pos_markers = [\n",
    "        'adj. inv.', 'adv. suff.', 'adj. suff. inv.', 'adj. suff.',\n",
    "        'n. suff.', 'n.suff.', 'm. suff.', 'f. suff.', \n",
    "        'v. t.', 'v. i.', 'v.i.', 'v.t.',\n",
    "        'adj.', 'adv.', 'pref.', 'suff.',\n",
    "        'conj.', 'post.', 'pron.', 'interj.', 'poss.',\n",
    "        'f.(i)', 'f.(e)', 'm.(i)', 'm.(e)',\n",
    "        'n.', 'm.', 'f.'\n",
    "    ]\n",
    "    \n",
    "    # Step 3: Find all POS markers and their positions\n",
    "    pos_locations = []\n",
    "    for pos in pos_markers:\n",
    "        index = 0\n",
    "        while True:\n",
    "            index = definition_text.find(pos, index)\n",
    "            if index == -1:\n",
    "                break\n",
    "            if index == 0 or definition_text[index-1] in [' ', '.']:\n",
    "                end_index = index + len(pos)\n",
    "                if end_index >= len(definition_text) or definition_text[end_index] == ' ':\n",
    "                    pos_locations.append({\n",
    "                        'pos_display': pos,\n",
    "                        'start': index,\n",
    "                        'end': end_index\n",
    "                    })\n",
    "            index += 1\n",
    "    \n",
    "    pos_locations.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    # Step 4: If no POS found, treat entire text as definition with null POS\n",
    "    if not pos_locations:\n",
    "        cleaned_text = definition_text.strip()\n",
    "        if cleaned_text.endswith('.'):\n",
    "            cleaned_text = cleaned_text[:-1].strip()\n",
    "        \n",
    "        if cleaned_text:\n",
    "            return [{\n",
    "                'definition': cleaned_text,\n",
    "                'pos_display': None,\n",
    "                'number': None,\n",
    "                'pos': None,\n",
    "                'gender': None,\n",
    "                'declension_class': None,\n",
    "                'referenced_entry': None\n",
    "            }]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    # Step 5: Split text into sections by POS\n",
    "    definitions = []\n",
    "    \n",
    "    for i, pos_info in enumerate(pos_locations):\n",
    "        current_pos = pos_info['pos_display']\n",
    "        start = pos_info['end']\n",
    "        \n",
    "        if i + 1 < len(pos_locations):\n",
    "            end = pos_locations[i + 1]['start']\n",
    "        else:\n",
    "            end = len(definition_text)\n",
    "        \n",
    "        section_text = definition_text[start:end].strip()\n",
    "        \n",
    "        # Step 6: Check if this section has numbered definitions\n",
    "        numbered_pattern = r'(\\d+)\\.\\s*([^0-9]+?)(?=\\s*\\d+\\.|$)'\n",
    "        numbered_matches = list(re.finditer(numbered_pattern, section_text))\n",
    "        \n",
    "        if numbered_matches:\n",
    "            for match in numbered_matches:\n",
    "                number = int(match.group(1))\n",
    "                def_text = match.group(2).strip()\n",
    "                \n",
    "                if def_text.endswith('.'):\n",
    "                    def_text = def_text[:-1].strip()\n",
    "                \n",
    "                if def_text:\n",
    "                    definitions.append({\n",
    "                        'definition': def_text,\n",
    "                        'pos_display': current_pos,\n",
    "                        'number': number,\n",
    "                        'pos': None,\n",
    "                        'gender': None,\n",
    "                        'declension_class': None,\n",
    "                        'referenced_entry': None\n",
    "                    })\n",
    "        else:\n",
    "            section_text = section_text.strip()\n",
    "            if section_text.endswith('.'):\n",
    "                section_text = section_text[:-1].strip()\n",
    "            \n",
    "            if section_text:\n",
    "                definitions.append({\n",
    "                    'definition': section_text,\n",
    "                    'pos_display': current_pos,\n",
    "                    'number': None,\n",
    "                    'pos': None,\n",
    "                    'gender': None,\n",
    "                    'declension_class': None,\n",
    "                    'referenced_entry': None\n",
    "                })\n",
    "    \n",
    "    return definitions\n",
    "\n",
    "\n",
    "# Add definitions to all entries\n",
    "for entry in data:\n",
    "    entry['definitions'] = parse_definitions(entry)\n",
    "\n",
    "# Calculate metrics\n",
    "total_entries = len(data)\n",
    "entries_with_defs = sum(1 for entry in data if entry['definitions'])\n",
    "entries_without_defs = total_entries - entries_with_defs\n",
    "total_defs = sum(len(e['definitions']) for e in data)\n",
    "entries_with_null_pos = sum(1 for entry in data if entry['definitions'] and any(d['pos_display'] is None for d in entry['definitions']))\n",
    "\n",
    "print(f\"Total entries: {total_entries}\")\n",
    "print(f\"Entries with definitions: {entries_with_defs} ({entries_with_defs/total_entries*100:.1f}%)\")\n",
    "print(f\"Entries without definitions: {entries_without_defs} ({entries_without_defs/total_entries*100:.1f}%)\")\n",
    "print(f\"Entries with null POS: {entries_with_null_pos} ({entries_with_null_pos/total_entries*100:.1f}%)\")\n",
    "print(f\"Total definitions parsed: {total_defs}\")\n",
    "print(f\"Average definitions per entry: {total_defs/total_entries:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "efbec9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized 11819 definitions with POS\n",
      "Definitions without POS: 348\n",
      "\n",
      "Example normalized entry:\n",
      "{\n",
      "  \"entry_id\": \"berntsen_अ_1\",\n",
      "  \"headword_devanagari\": \"अ\",\n",
      "  \"headword_romanized\": \"a\",\n",
      "  \"full_entry\": \"अ a pref. negative.\",\n",
      "  \"source_page\": 1,\n",
      "  \"entry_type\": \"headword\",\n",
      "  \"base_word\": null,\n",
      "  \"search_text\": null,\n",
      "  \"definitions\": [\n",
      "    {\n",
      "      \"definition\": \"negative\",\n",
      "      \"pos_display\": \"pref.\",\n",
      "      \"number\": null,\n",
      "      \"pos\": \"prefix\",\n",
      "      \"gender\": null,\n",
      "      \"declension_class\": null,\n",
      "      \"referenced_entry\": null\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: Normalize POS and Extract Gender\n",
    "# ==============================================================================\n",
    "# This cell:\n",
    "# - Converts pos_display (e.g., 'm.', 'f.(i)') to standardized pos (e.g., 'noun.masculine')\n",
    "# - Extracts gender from noun POS markers\n",
    "# - Extracts declension_class from markers like 'f.(i)', 'f.(e)'\n",
    "\n",
    "def normalize_pos(pos_display):\n",
    "    \"\"\"\n",
    "    Convert display POS to standardized POS, gender, and declension class.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (pos, gender, declension_class)\n",
    "    \"\"\"\n",
    "    if pos_display is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    pos_mapping = {\n",
    "        # Nouns with gender\n",
    "        'm.': ('noun.masculine', 'masculine', None),\n",
    "        'f.': ('noun.feminine', 'feminine', None),\n",
    "        'n.': ('noun.neuter', 'neuter', None),\n",
    "        \n",
    "        # Nouns with declension class\n",
    "        'f.(i)': ('noun.feminine.i', 'feminine', 'i'),\n",
    "        'f.(e)': ('noun.feminine.e', 'feminine', 'e'),\n",
    "        'm.(i)': ('noun.masculine.i', 'masculine', 'i'),\n",
    "        'm.(e)': ('noun.masculine.e', 'masculine', 'e'),\n",
    "        \n",
    "        # Verbs\n",
    "        'v.t.': ('verb.transitive', None, None),\n",
    "        'v. t.': ('verb.transitive', None, None),\n",
    "        'v.i.': ('verb.intransitive', None, None),\n",
    "        'v. i.': ('verb.intransitive', None, None),\n",
    "        \n",
    "        # Adjectives\n",
    "        'adj.': ('adjective', None, None),\n",
    "        'adj. inv.': ('adjective.invariant', None, None),\n",
    "        'adj. suff.': ('adjective.suffix', None, None),\n",
    "        'adj. suff. inv.': ('adjective.suffix.invariant', None, None),\n",
    "        \n",
    "        # Adverbs\n",
    "        'adv.': ('adverb', None, None),\n",
    "        'adv. suff.': ('adverb.suffix', None, None),\n",
    "        \n",
    "        # Other POS\n",
    "        'pref.': ('prefix', None, None),\n",
    "        'suff.': ('suffix', None, None),\n",
    "        'n. suff.': ('noun.suffix', None, None),\n",
    "        'n.suff.': ('noun.suffix', None, None),\n",
    "        'm. suff.': ('noun.masculine.suffix', 'masculine', None),\n",
    "        'f. suff.': ('noun.feminine.suffix', 'feminine', None),\n",
    "        'conj.': ('conjunction', None, None),\n",
    "        'post.': ('postposition', None, None),\n",
    "        'pron.': ('pronoun', None, None),\n",
    "        'interj.': ('interjection', None, None),\n",
    "        'poss.': ('possessive', None, None),\n",
    "    }\n",
    "    \n",
    "    return pos_mapping.get(pos_display, (None, None, None))\n",
    "\n",
    "\n",
    "# Apply normalization to all definitions\n",
    "normalized_count = 0\n",
    "for entry in data:\n",
    "    for defn in entry['definitions']:\n",
    "        pos, gender, declension = normalize_pos(defn['pos_display'])\n",
    "        defn['pos'] = pos\n",
    "        defn['gender'] = gender\n",
    "        defn['declension_class'] = declension\n",
    "        if pos is not None:\n",
    "            normalized_count += 1\n",
    "\n",
    "print(f\"Normalized {normalized_count} definitions with POS\")\n",
    "print(f\"Definitions without POS: {total_defs - normalized_count}\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample normalized entry:\")\n",
    "example = next(e for e in data if e['definitions'] and e['definitions'][0]['pos'])\n",
    "print(json.dumps(example, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f0eb834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-references processed: 81\n",
      "Cross-references not found: 2\n",
      "\n",
      "Cross-references not found (first 10):\n",
      "  भोकणे -> भुकंणे\n",
      "  सुरुवात -> सुरवात\n",
      "\n",
      "Example cross-reference:\n",
      "{\n",
      "  \"entry_id\": \"berntsen_अलीकडे_5\",\n",
      "  \"headword_devanagari\": \"अलीकडे\",\n",
      "  \"headword_romanized\": \"alīkaḍē\",\n",
      "  \"full_entry\": \"अलीकडे alīkaḍē See अलिकडे .\",\n",
      "  \"source_page\": 5,\n",
      "  \"entry_type\": \"headword\",\n",
      "  \"base_word\": null,\n",
      "  \"search_text\": null,\n",
      "  \"definitions\": [\n",
      "    {\n",
      "      \"definition\": \"See अलिकडे\",\n",
      "      \"pos_display\": null,\n",
      "      \"number\": null,\n",
      "      \"pos\": null,\n",
      "      \"gender\": null,\n",
      "      \"declension_class\": null,\n",
      "      \"referenced_entry\": {\n",
      "        \"relationship\": \"cross_reference\",\n",
      "        \"grammatical_form\": null,\n",
      "        \"entry_id\": \"berntsen_अलिकडे_5\",\n",
      "        \"headword_devanagari\": \"अलिकडे\",\n",
      "        \"headword_romanized\": \"alikaḍē\",\n",
      "        \"full_entry\": \"अलिकडे alikaḍē adv. 1. on this side. 2. recently. post. on this side.\",\n",
      "        \"source_page\": 5,\n",
      "        \"definitions\": [\n",
      "          {\n",
      "            \"definition\": \"on this side\",\n",
      "            \"pos_display\": \"adv.\",\n",
      "            \"number\": 1,\n",
      "            \"pos\": \"adverb\",\n",
      "            \"gender\": null,\n",
      "            \"declension_class\": null,\n",
      "            \"referenced_entry\": null\n",
      "          },\n",
      "          {\n",
      "            \"definition\": \"recently\",\n",
      "            \"pos_display\": \"adv.\",\n",
      "            \"number\": 2,\n",
      "            \"pos\": \"adverb\",\n",
      "            \"gender\": null,\n",
      "            \"declension_class\": null,\n",
      "            \"referenced_entry\": null\n",
      "          },\n",
      "          {\n",
      "            \"definition\": \"on this side\",\n",
      "            \"pos_display\": \"post.\",\n",
      "            \"number\": null,\n",
      "            \"pos\": \"postposition\",\n",
      "            \"gender\": null,\n",
      "            \"declension_class\": null,\n",
      "            \"referenced_entry\": null\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: Process Cross-References (\"See X\")\n",
    "# ==============================================================================\n",
    "# This cell:\n",
    "# - Finds definitions that are cross-references (e.g., \"See हुजत\")\n",
    "# - Looks up the referenced entry in the dictionary\n",
    "# - Populates referenced_entry with relationship: 'cross_reference'\n",
    "# - Embeds the full referenced entry for self-contained RAG chunks\n",
    "\n",
    "import re\n",
    "\n",
    "def find_entry_by_headword(headword_devanagari, data):\n",
    "    \"\"\"Find an entry by its Devanagari headword.\"\"\"\n",
    "    for entry in data:\n",
    "        if entry['headword_devanagari'] == headword_devanagari:\n",
    "            return entry\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_referenced_entry(source_entry, relationship, grammatical_form=None):\n",
    "    \"\"\"\n",
    "    Create a referenced_entry object from a source entry.\n",
    "    \n",
    "    Args:\n",
    "        source_entry: The entry being referenced\n",
    "        relationship: Type of relationship (cross_reference, base_form, etc.)\n",
    "        grammatical_form: Optional grammatical form (perfective, plural, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        OrderedDict with referenced entry structure\n",
    "    \"\"\"\n",
    "    if source_entry is None:\n",
    "        return None\n",
    "    \n",
    "    ref = OrderedDict()\n",
    "    ref['relationship'] = relationship\n",
    "    ref['grammatical_form'] = grammatical_form\n",
    "    ref['entry_id'] = source_entry['entry_id']\n",
    "    ref['headword_devanagari'] = source_entry['headword_devanagari']\n",
    "    ref['headword_romanized'] = source_entry['headword_romanized']\n",
    "    ref['full_entry'] = source_entry['full_entry']\n",
    "    ref['source_page'] = source_entry['source_page']\n",
    "    ref['definitions'] = deepcopy(source_entry['definitions'])\n",
    "    \n",
    "    return ref\n",
    "\n",
    "\n",
    "# Pattern to match \"See X\" where X is Devanagari\n",
    "# Also handles \"See X , def. 1\" format\n",
    "see_pattern = re.compile(r'^See\\s+([\\u0900-\\u097F]+)(?:\\s*,\\s*def\\.\\s*(\\d+))?\\.?$')\n",
    "\n",
    "cross_ref_count = 0\n",
    "cross_ref_not_found = []\n",
    "\n",
    "for entry in data:\n",
    "    for defn in entry['definitions']:\n",
    "        match = see_pattern.match(defn['definition'])\n",
    "        if match:\n",
    "            referenced_headword = match.group(1)\n",
    "            def_number = match.group(2)  # May be None\n",
    "            \n",
    "            # Find the referenced entry\n",
    "            referenced_entry = find_entry_by_headword(referenced_headword, data)\n",
    "            \n",
    "            if referenced_entry:\n",
    "                defn['referenced_entry'] = create_referenced_entry(\n",
    "                    referenced_entry, \n",
    "                    relationship='cross_reference'\n",
    "                )\n",
    "                cross_ref_count += 1\n",
    "            else:\n",
    "                cross_ref_not_found.append({\n",
    "                    'entry': entry['headword_devanagari'],\n",
    "                    'looking_for': referenced_headword\n",
    "                })\n",
    "\n",
    "print(f\"Cross-references processed: {cross_ref_count}\")\n",
    "print(f\"Cross-references not found: {len(cross_ref_not_found)}\")\n",
    "\n",
    "if cross_ref_not_found:\n",
    "    print(\"\\nCross-references not found (first 10):\")\n",
    "    for item in cross_ref_not_found[:10]:\n",
    "        print(f\"  {item['entry']} -> {item['looking_for']}\")\n",
    "\n",
    "# Verify with example\n",
    "print(\"\\nExample cross-reference:\")\n",
    "example = next((e for e in data if e['definitions'] and \n",
    "                e['definitions'][0].get('referenced_entry') and\n",
    "                e['definitions'][0]['referenced_entry']['relationship'] == 'cross_reference'), None)\n",
    "if example:\n",
    "    print(json.dumps(example, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f1ab2896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammatical forms processed: 39\n",
      "Grammatical forms not found: 0\n",
      "\n",
      "Grammatical forms by type:\n",
      "  perfective: 18\n",
      "  possessive_of: 8\n",
      "  plural: 3\n",
      "  oblique: 3\n",
      "  past_tense: 2\n",
      "  present_tense: 1\n",
      "  instrumental: 1\n",
      "  vocative_singular: 1\n",
      "  negative: 1\n",
      "  plural_oblique: 1\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 5: Process Grammatical Forms\n",
    "# ==============================================================================\n",
    "# This cell:\n",
    "# - Finds definitions that reference base forms (e.g., \"perf. of गाणे\")\n",
    "# - Handles: perf. of, past tense of, pres. of, hab. of, negative of,\n",
    "#   obl. of, instr. of, loc. of, voc. sg. of, pl. of\n",
    "# - Populates referenced_entry with relationship: 'base_form'\n",
    "# - Includes grammatical_form field (e.g., 'perfective', 'plural')\n",
    "\n",
    "import re\n",
    "\n",
    "# Pattern to match grammatical form references\n",
    "# Captures: (grammatical indicator) of (Devanagari word)\n",
    "grammatical_patterns = [\n",
    "    (r'^perf\\.\\s+of\\s+([\\u0900-\\u097F]+)\\.?$', 'perfective'),\n",
    "    (r'^past tense of\\s+([\\u0900-\\u097F]+)\\.?$', 'past_tense'),\n",
    "    (r'^pres\\.\\s+of\\s+([\\u0900-\\u097F]+)\\.?$', 'present_tense'),\n",
    "    (r'^hab\\.\\s+of\\s+([\\u0900-\\u097F]+)\\.?$', 'habitual'),\n",
    "    (r'^negative of\\s+([\\u0900-\\u097F]+)\\.?$', 'negative'),\n",
    "    (r'^obl\\.?\\s*(?:form\\s+)?of\\s+([\\u0900-\\u097F]+)\\.?$', 'oblique'),\n",
    "    (r'^instr\\.\\s+of\\s+([\\u0900-\\u097F]+)\\.?$', 'instrumental'),\n",
    "    (r'^loc\\.\\s+of\\s+([\\u0900-\\u097F]+)\\.?$', 'locative'),\n",
    "    (r'^voc\\.\\s+sg\\.\\s+of\\s+([\\u0900-\\u097F]+)\\.?$', 'vocative_singular'),\n",
    "    (r'^pl\\.\\s+of\\s+([\\u0900-\\u097F]+)\\.?$', 'plural'),\n",
    "    (r'^pl\\.\\s+and\\s+obl\\.\\s+of\\s+([\\u0900-\\u097F]+)\\.?$', 'plural_oblique'),\n",
    "    (r'^of\\s+([\\u0900-\\u097F]+)$', 'possessive_of'),  # For possessives like \"of आम्ही\"\n",
    "]\n",
    "\n",
    "# Compile patterns\n",
    "compiled_patterns = [(re.compile(pattern, re.IGNORECASE), form) for pattern, form in grammatical_patterns]\n",
    "\n",
    "grammatical_count = 0\n",
    "grammatical_not_found = []\n",
    "\n",
    "for entry in data:\n",
    "    for defn in entry['definitions']:\n",
    "        # Skip if already has a referenced_entry (e.g., cross-reference)\n",
    "        if defn['referenced_entry'] is not None:\n",
    "            continue\n",
    "        \n",
    "        definition = defn['definition']\n",
    "        \n",
    "        for pattern, grammatical_form in compiled_patterns:\n",
    "            match = pattern.match(definition)\n",
    "            if match:\n",
    "                referenced_headword = match.group(1)\n",
    "                \n",
    "                # Find the referenced entry\n",
    "                referenced_entry = find_entry_by_headword(referenced_headword, data)\n",
    "                \n",
    "                if referenced_entry:\n",
    "                    defn['referenced_entry'] = create_referenced_entry(\n",
    "                        referenced_entry,\n",
    "                        relationship='base_form',\n",
    "                        grammatical_form=grammatical_form\n",
    "                    )\n",
    "                    grammatical_count += 1\n",
    "                else:\n",
    "                    grammatical_not_found.append({\n",
    "                        'entry': entry['headword_devanagari'],\n",
    "                        'looking_for': referenced_headword,\n",
    "                        'form': grammatical_form\n",
    "                    })\n",
    "                break  # Found a match, no need to check other patterns\n",
    "\n",
    "print(f\"Grammatical forms processed: {grammatical_count}\")\n",
    "print(f\"Grammatical forms not found: {len(grammatical_not_found)}\")\n",
    "\n",
    "if grammatical_not_found:\n",
    "    print(\"\\nGrammatical forms not found (first 10):\")\n",
    "    for item in grammatical_not_found[:10]:\n",
    "        print(f\"  {item['entry']} -> {item['looking_for']} ({item['form']})\")\n",
    "\n",
    "# Show counts by grammatical form\n",
    "form_counts = {}\n",
    "for entry in data:\n",
    "    for defn in entry['definitions']:\n",
    "        ref = defn.get('referenced_entry')\n",
    "        if ref and ref['relationship'] == 'base_form':\n",
    "            form = ref['grammatical_form']\n",
    "            form_counts[form] = form_counts.get(form, 0) + 1\n",
    "\n",
    "print(\"\\nGrammatical forms by type:\")\n",
    "for form, count in sorted(form_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {form}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f157f18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduplications processed: 16\n",
      "Reduplications not found: 1\n",
      "\n",
      "Reduplications not found:\n",
      "  निजानीज -> नीज\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 6: Process Reduplications\n",
    "# ==============================================================================\n",
    "# This cell:\n",
    "# - Finds definitions that are reduplications (e.g., \"redupl. of खरे\")\n",
    "# - Handles: redupl. of, redupl. loc. of\n",
    "# - Populates referenced_entry with relationship: 'reduplication_of'\n",
    "\n",
    "import re\n",
    "\n",
    "# Patterns for reduplication\n",
    "redupl_patterns = [\n",
    "    (r'^redupl\\.\\s+of\\s+([\\u0900-\\u097F]+)\\.?(?:\\s+.*)?$', None),\n",
    "    (r'^redupl\\.\\s+loc\\.\\s+of\\s+([\\u0900-\\u097F]+)\\.?(?:\\s+.*)?$', 'locative'),\n",
    "]\n",
    "\n",
    "compiled_redupl = [(re.compile(pattern, re.IGNORECASE), form) for pattern, form in redupl_patterns]\n",
    "\n",
    "redupl_count = 0\n",
    "redupl_not_found = []\n",
    "\n",
    "for entry in data:\n",
    "    for defn in entry['definitions']:\n",
    "        # Skip if already has a referenced_entry\n",
    "        if defn['referenced_entry'] is not None:\n",
    "            continue\n",
    "        \n",
    "        definition = defn['definition']\n",
    "        \n",
    "        for pattern, grammatical_form in compiled_redupl:\n",
    "            match = pattern.match(definition)\n",
    "            if match:\n",
    "                referenced_headword = match.group(1)\n",
    "                \n",
    "                referenced_entry = find_entry_by_headword(referenced_headword, data)\n",
    "                \n",
    "                if referenced_entry:\n",
    "                    defn['referenced_entry'] = create_referenced_entry(\n",
    "                        referenced_entry,\n",
    "                        relationship='reduplication_of',\n",
    "                        grammatical_form=grammatical_form\n",
    "                    )\n",
    "                    redupl_count += 1\n",
    "                else:\n",
    "                    redupl_not_found.append({\n",
    "                        'entry': entry['headword_devanagari'],\n",
    "                        'looking_for': referenced_headword\n",
    "                    })\n",
    "                break\n",
    "\n",
    "print(f\"Reduplications processed: {redupl_count}\")\n",
    "print(f\"Reduplications not found: {len(redupl_not_found)}\")\n",
    "\n",
    "if redupl_not_found:\n",
    "    print(\"\\nReduplications not found:\")\n",
    "    for item in redupl_not_found:\n",
    "        print(f\"  {item['entry']} -> {item['looking_for']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a9fffcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviations processed: 4\n",
      "Abbreviations not found: 0\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 7: Process Abbreviations\n",
    "# ==============================================================================\n",
    "# This cell:\n",
    "# - Finds definitions that are abbreviations (e.g., \"abbrev. of इत्यादी\")\n",
    "# - Populates referenced_entry with relationship: 'abbreviation_of'\n",
    "\n",
    "import re\n",
    "\n",
    "# Pattern for abbreviations\n",
    "abbrev_pattern = re.compile(r'^abbrev\\.?\\s+of\\.?\\s+([\\u0900-\\u097F]+)\\.?$', re.IGNORECASE)\n",
    "\n",
    "abbrev_count = 0\n",
    "abbrev_not_found = []\n",
    "\n",
    "for entry in data:\n",
    "    for defn in entry['definitions']:\n",
    "        # Skip if already has a referenced_entry\n",
    "        if defn['referenced_entry'] is not None:\n",
    "            continue\n",
    "        \n",
    "        definition = defn['definition']\n",
    "        match = abbrev_pattern.match(definition)\n",
    "        \n",
    "        if match:\n",
    "            referenced_headword = match.group(1)\n",
    "            \n",
    "            referenced_entry = find_entry_by_headword(referenced_headword, data)\n",
    "            \n",
    "            if referenced_entry:\n",
    "                defn['referenced_entry'] = create_referenced_entry(\n",
    "                    referenced_entry,\n",
    "                    relationship='abbreviation_of'\n",
    "                )\n",
    "                abbrev_count += 1\n",
    "            else:\n",
    "                abbrev_not_found.append({\n",
    "                    'entry': entry['headword_devanagari'],\n",
    "                    'looking_for': referenced_headword\n",
    "                })\n",
    "\n",
    "print(f\"Abbreviations processed: {abbrev_count}\")\n",
    "print(f\"Abbreviations not found: {len(abbrev_not_found)}\")\n",
    "\n",
    "if abbrev_not_found:\n",
    "    print(\"\\nAbbreviations not found:\")\n",
    "    for item in abbrev_not_found:\n",
    "        print(f\"  {item['entry']} -> {item['looking_for']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6b1f998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variants processed: 0\n",
      "Variants not found: 1\n",
      "\n",
      "Variants not found:\n",
      "  मज -> माझ्या\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 8: Process Variants\n",
    "# ==============================================================================\n",
    "# This cell:\n",
    "# - Finds definitions that are variants (e.g., \"var. of माझ्या\")\n",
    "# - Populates referenced_entry with relationship: 'variant_of'\n",
    "\n",
    "import re\n",
    "\n",
    "# Pattern for variants\n",
    "variant_pattern = re.compile(r'^var\\.?\\s+of\\s+([\\u0900-\\u097F]+)\\.?$', re.IGNORECASE)\n",
    "\n",
    "variant_count = 0\n",
    "variant_not_found = []\n",
    "\n",
    "for entry in data:\n",
    "    for defn in entry['definitions']:\n",
    "        # Skip if already has a referenced_entry\n",
    "        if defn['referenced_entry'] is not None:\n",
    "            continue\n",
    "        \n",
    "        definition = defn['definition']\n",
    "        match = variant_pattern.match(definition)\n",
    "        \n",
    "        if match:\n",
    "            referenced_headword = match.group(1)\n",
    "            \n",
    "            referenced_entry = find_entry_by_headword(referenced_headword, data)\n",
    "            \n",
    "            if referenced_entry:\n",
    "                defn['referenced_entry'] = create_referenced_entry(\n",
    "                    referenced_entry,\n",
    "                    relationship='variant_of'\n",
    "                )\n",
    "                variant_count += 1\n",
    "            else:\n",
    "                variant_not_found.append({\n",
    "                    'entry': entry['headword_devanagari'],\n",
    "                    'looking_for': referenced_headword\n",
    "                })\n",
    "\n",
    "print(f\"Variants processed: {variant_count}\")\n",
    "print(f\"Variants not found: {len(variant_not_found)}\")\n",
    "\n",
    "if variant_not_found:\n",
    "    print(\"\\nVariants not found:\")\n",
    "    for item in variant_not_found:\n",
    "        print(f\"  {item['entry']} -> {item['looking_for']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "83d77908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries containing collocations: 486\n",
      "Collocation entries created: 624\n",
      "Total entries now: 10460\n",
      "\n",
      "Example collocation entries:\n",
      "  अखेर चे: \"final\"\n",
      "    Base: अखेर (end; finally...)\n",
      "  अंग चे: \"natural, inborn\"\n",
      "    Base: अंग (body; part; side; capacity, skill...)\n",
      "  अंग चोरणे: \"to contract one's body, shirk\"\n",
      "    Base: अंग (body; part; side; capacity, skill...)\n",
      "  अंग धरणे: \"to have rheumatic pains, gain weight\"\n",
      "    Base: अंग (body; part; side; capacity, skill...)\n",
      "  अंग वळणी असणे to be used to. अंगात येणे: \"to be possessed (by a god, evil spirit)\"\n",
      "    Base: अंग (body; part; side; capacity, skill...)\n",
      "\n",
      "--- Quality Check ---\n",
      "Collocations with very short meanings (<3 chars): 0\n",
      "Collocations with Devanagari in meaning: 0\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 9: Process Collocations (Create New Entries) - FIXED VERSION\n",
    "# ==============================================================================\n",
    "# This cell:\n",
    "# - Finds definitions with collocation patterns (e.g., \"० करणे to urge\")\n",
    "# - Creates NEW entries for each collocation with entry_type: 'collocation'\n",
    "# - Populates base_word with the parent entry's information\n",
    "# - Handles complex patterns:\n",
    "#   1. Simple: '० करणे to urge'\n",
    "#   2. Multi-verb same meaning: '० पडणे , पाडणे , घालणे to frown'\n",
    "#   3. Multi-० same meaning: '० होणे , ० पावणे to contract'\n",
    "#   4. Multiple separate collocations: '० करणे to X. ० होणे to Y'\n",
    "#   5. Attributive: '० चे final'\n",
    "#   6. Multi-word verbs: '० करून देणे to introduce'\n",
    "# - Adds new collocation entries to the data list\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def parse_collocations(definition_text, headword_devanagari):\n",
    "    \"\"\"\n",
    "    Parse collocations from a definition string containing ० patterns.\n",
    "    \n",
    "    Args:\n",
    "        definition_text: The definition string containing ० markers\n",
    "        headword_devanagari: The headword in Devanagari script\n",
    "    \n",
    "    Returns:\n",
    "        List of collocation dicts with: phrase_devanagari, verb, meaning\n",
    "    \"\"\"\n",
    "    collocations = []\n",
    "    \n",
    "    if '०' not in definition_text:\n",
    "        return []\n",
    "    \n",
    "    # Split on ० to get segments\n",
    "    parts = definition_text.split('०')\n",
    "    \n",
    "    # Skip the first part (base definition before first ०)\n",
    "    segments = parts[1:]\n",
    "    \n",
    "    for idx, segment in enumerate(segments):\n",
    "        segment = segment.strip()\n",
    "        if not segment:\n",
    "            continue\n",
    "        \n",
    "        # Check if segment ends with comma (indicates verb is pending, meaning comes later)\n",
    "        segment_stripped = segment.rstrip()\n",
    "        ends_with_comma = segment_stripped.endswith(',')\n",
    "        \n",
    "        if ends_with_comma:\n",
    "            segment_work = segment_stripped[:-1].strip()\n",
    "        else:\n",
    "            segment_work = segment_stripped\n",
    "        \n",
    "        # Find the boundary between Devanagari and English\n",
    "        # Everything up to last Devanagari char is the verb part\n",
    "        # Everything after is the meaning\n",
    "        last_deva_pos = -1\n",
    "        for i, char in enumerate(segment_work):\n",
    "            if '\\u0900' <= char <= '\\u097F':\n",
    "                last_deva_pos = i\n",
    "        \n",
    "        if last_deva_pos == -1:\n",
    "            # No Devanagari in segment, skip\n",
    "            continue\n",
    "        \n",
    "        deva_part = segment_work[:last_deva_pos + 1].strip()\n",
    "        meaning_part = segment_work[last_deva_pos + 1:].strip()\n",
    "        \n",
    "        # Clean up meaning\n",
    "        meaning_part = meaning_part.strip().lstrip('.').strip()\n",
    "        if meaning_part.endswith('.'):\n",
    "            meaning_part = meaning_part[:-1].strip()\n",
    "        \n",
    "        # Handle comma-separated verbs (e.g., 'पडणे , पाडणे , घालणे')\n",
    "        if ',' in deva_part:\n",
    "            verbs = [v.strip() for v in deva_part.split(',') if v.strip()]\n",
    "        else:\n",
    "            verbs = [deva_part] if deva_part else []\n",
    "        \n",
    "        # Determine meaning\n",
    "        if meaning_part:\n",
    "            meaning = meaning_part\n",
    "        elif ends_with_comma:\n",
    "            # Look ahead for meaning in next segment(s)\n",
    "            meaning = None\n",
    "            for future_idx in range(idx + 1, len(segments)):\n",
    "                future_seg = segments[future_idx].strip()\n",
    "                # Find meaning in future segment\n",
    "                last_deva = -1\n",
    "                for i, char in enumerate(future_seg):\n",
    "                    if '\\u0900' <= char <= '\\u097F':\n",
    "                        last_deva = i\n",
    "                if last_deva >= 0:\n",
    "                    future_meaning = future_seg[last_deva + 1:].strip().lstrip('.').strip()\n",
    "                    if future_meaning:\n",
    "                        meaning = future_meaning.rstrip('.').strip()\n",
    "                        break\n",
    "        else:\n",
    "            meaning = None\n",
    "        \n",
    "        # Create collocations for each verb\n",
    "        for verb in verbs:\n",
    "            if verb and meaning:\n",
    "                phrase = f'{headword_devanagari} {verb}'\n",
    "                collocations.append({\n",
    "                    'phrase_devanagari': phrase,\n",
    "                    'verb': verb,\n",
    "                    'meaning': meaning\n",
    "                })\n",
    "    \n",
    "    return collocations\n",
    "\n",
    "\n",
    "def get_clean_base_definition(entry):\n",
    "    \"\"\"\n",
    "    Extract a clean base definition from entry, removing collocation patterns.\n",
    "    \n",
    "    Args:\n",
    "        entry: The dictionary entry\n",
    "    \n",
    "    Returns:\n",
    "        String with cleaned definition text\n",
    "    \"\"\"\n",
    "    definitions = []\n",
    "    for defn in entry['definitions']:\n",
    "        def_text = defn['definition']\n",
    "        # Remove everything from first ० onwards for cleaner base definition\n",
    "        if '०' in def_text:\n",
    "            clean_part = def_text.split('०')[0].strip()\n",
    "            if clean_part:\n",
    "                definitions.append(clean_part.rstrip('.').strip())\n",
    "        else:\n",
    "            definitions.append(def_text)\n",
    "    \n",
    "    result = '; '.join(definitions)\n",
    "    # Clean up any trailing punctuation or whitespace\n",
    "    result = re.sub(r'[;\\s]+$', '', result)\n",
    "    return result if result else '; '.join(d['definition'] for d in entry['definitions'])\n",
    "\n",
    "\n",
    "# Track statistics\n",
    "collocation_entries_created = 0\n",
    "entries_with_collocations = 0\n",
    "\n",
    "# Store new collocation entries to add\n",
    "new_collocation_entries = []\n",
    "\n",
    "# Track which collocations we've created to avoid duplicates\n",
    "seen_collocations = set()\n",
    "\n",
    "for entry in data:\n",
    "    entry_has_collocation = False\n",
    "    entry_collocations = []\n",
    "    \n",
    "    for defn in entry['definitions']:\n",
    "        definition = defn['definition']\n",
    "        \n",
    "        # Check if definition contains ०\n",
    "        if '०' not in definition:\n",
    "            continue\n",
    "        \n",
    "        entry_has_collocation = True\n",
    "        \n",
    "        # Parse collocations from this definition\n",
    "        collocations = parse_collocations(\n",
    "            definition,\n",
    "            entry['headword_devanagari']\n",
    "        )\n",
    "        \n",
    "        entry_collocations.extend(collocations)\n",
    "    \n",
    "    # Create new entries for unique collocations\n",
    "    colloc_counter = 1\n",
    "    for colloc in entry_collocations:\n",
    "        # Create unique key to avoid duplicates\n",
    "        colloc_key = (entry['entry_id'], colloc['phrase_devanagari'])\n",
    "        if colloc_key in seen_collocations:\n",
    "            continue\n",
    "        seen_collocations.add(colloc_key)\n",
    "        \n",
    "        # Skip collocations with very short/invalid meanings\n",
    "        if len(colloc['meaning']) < 2:\n",
    "            continue\n",
    "        \n",
    "        new_entry = OrderedDict()\n",
    "        \n",
    "        # Generate unique entry_id\n",
    "        new_entry['entry_id'] = f\"{entry['entry_id']}_c{colloc_counter}\"\n",
    "        new_entry['headword_devanagari'] = colloc['phrase_devanagari']\n",
    "        new_entry['headword_romanized'] = None  # Would need transliteration\n",
    "        new_entry['full_entry'] = f\"{colloc['phrase_devanagari']} {colloc['meaning']}\"\n",
    "        new_entry['source_page'] = entry['source_page']\n",
    "        new_entry['entry_type'] = 'collocation'\n",
    "        \n",
    "        # Create definition for the collocation\n",
    "        new_entry['definitions'] = [{\n",
    "            'definition': colloc['meaning'],\n",
    "            'pos_display': None,\n",
    "            'number': None,\n",
    "            'pos': None,\n",
    "            'gender': None,\n",
    "            'declension_class': None,\n",
    "            'referenced_entry': None\n",
    "        }]\n",
    "        \n",
    "        # Populate base_word with parent entry info\n",
    "        base_def_clean = get_clean_base_definition(entry)\n",
    "        \n",
    "        new_entry['base_word'] = OrderedDict()\n",
    "        new_entry['base_word']['entry_id'] = entry['entry_id']\n",
    "        new_entry['base_word']['headword_devanagari'] = entry['headword_devanagari']\n",
    "        new_entry['base_word']['headword_romanized'] = entry['headword_romanized']\n",
    "        new_entry['base_word']['definition'] = base_def_clean\n",
    "        new_entry['base_word']['pos'] = entry['definitions'][0]['pos'] if entry['definitions'] else None\n",
    "        \n",
    "        new_entry['search_text'] = None  # Computed later in Cell 10\n",
    "        \n",
    "        new_collocation_entries.append(new_entry)\n",
    "        collocation_entries_created += 1\n",
    "        colloc_counter += 1\n",
    "    \n",
    "    if entry_has_collocation:\n",
    "        entries_with_collocations += 1\n",
    "\n",
    "# Add new collocation entries to data\n",
    "data.extend(new_collocation_entries)\n",
    "\n",
    "print(f\"Entries containing collocations: {entries_with_collocations}\")\n",
    "print(f\"Collocation entries created: {collocation_entries_created}\")\n",
    "print(f\"Total entries now: {len(data)}\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample collocation entries:\")\n",
    "colloc_examples = [e for e in data if e['entry_type'] == 'collocation'][:5]\n",
    "for ex in colloc_examples:\n",
    "    print(f\"  {ex['headword_devanagari']}: \\\"{ex['definitions'][0]['definition']}\\\"\")\n",
    "    print(f\"    Base: {ex['base_word']['headword_devanagari']} ({ex['base_word']['definition'][:40]}...)\")\n",
    "\n",
    "# Verify quality\n",
    "print(\"\\n--- Quality Check ---\")\n",
    "short_meanings = [e for e in data if e['entry_type'] == 'collocation' and len(e['definitions'][0]['definition']) < 3]\n",
    "print(f\"Collocations with very short meanings (<3 chars): {len(short_meanings)}\")\n",
    "\n",
    "deva_pattern = re.compile(r'[\\u0900-\\u097F]')\n",
    "deva_in_meaning = [e for e in data if e['entry_type'] == 'collocation' and deva_pattern.search(e['definitions'][0]['definition'])]\n",
    "print(f\"Collocations with Devanagari in meaning: {len(deva_in_meaning)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "00885705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_text built for all entries\n",
      "\n",
      "Total entries: 10460\n",
      "\n",
      "================================================================================\n",
      "Example search_text (headword entry):\n",
      "================================================================================\n",
      "Entry: अ\n",
      "search_text: अ a headword negative prefix...\n",
      "\n",
      "================================================================================\n",
      "Example search_text (entry with referenced_entry):\n",
      "================================================================================\n",
      "Entry: अलीकडे\n",
      "search_text: अलीकडे alīkaḍē headword See अलिकडे cross_reference अलिकडे alikaḍē on this side adverb recently adverb on this side postposition...\n",
      "\n",
      "================================================================================\n",
      "Example search_text (collocation entry):\n",
      "================================================================================\n",
      "Entry: अखेर चे\n",
      "search_text: अखेर चे collocation final अखेर akhēra end; finally noun.feminine.i\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 10: Build search_text\n",
    "# ==============================================================================\n",
    "# This cell:\n",
    "# - Constructs the search_text field for each entry\n",
    "# - Concatenates all relevant fields for vector embedding\n",
    "# - For entries with referenced_entry, includes the referenced entry's info\n",
    "# - For collocations, includes base_word info\n",
    "\n",
    "def build_search_text(entry):\n",
    "    \"\"\"\n",
    "    Build a comprehensive search_text for vector embedding.\n",
    "    \n",
    "    Includes:\n",
    "    - Headword (Devanagari and romanized)\n",
    "    - All definitions\n",
    "    - POS information\n",
    "    - Referenced entry info (if present)\n",
    "    - Base word info (for collocations)\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Add headword\n",
    "    parts.append(entry['headword_devanagari'])\n",
    "    if entry['headword_romanized']:\n",
    "        parts.append(entry['headword_romanized'])\n",
    "    \n",
    "    # Add entry type\n",
    "    parts.append(entry['entry_type'])\n",
    "    \n",
    "    # Add definitions and their metadata\n",
    "    for defn in entry['definitions']:\n",
    "        parts.append(defn['definition'])\n",
    "        \n",
    "        if defn['pos']:\n",
    "            parts.append(defn['pos'])\n",
    "        \n",
    "        if defn['gender']:\n",
    "            parts.append(defn['gender'])\n",
    "        \n",
    "        # Add referenced entry info\n",
    "        ref = defn.get('referenced_entry')\n",
    "        if ref:\n",
    "            parts.append(ref['relationship'])\n",
    "            if ref['grammatical_form']:\n",
    "                parts.append(ref['grammatical_form'])\n",
    "            parts.append(ref['headword_devanagari'])\n",
    "            if ref['headword_romanized']:\n",
    "                parts.append(ref['headword_romanized'])\n",
    "            # Add referenced definitions\n",
    "            for ref_defn in ref.get('definitions', []):\n",
    "                parts.append(ref_defn['definition'])\n",
    "                if ref_defn.get('pos'):\n",
    "                    parts.append(ref_defn['pos'])\n",
    "    \n",
    "    # Add base_word info for collocations\n",
    "    base = entry.get('base_word')\n",
    "    if base:\n",
    "        parts.append(base['headword_devanagari'])\n",
    "        if base['headword_romanized']:\n",
    "            parts.append(base['headword_romanized'])\n",
    "        parts.append(base['definition'])\n",
    "        if base['pos']:\n",
    "            parts.append(base['pos'])\n",
    "    \n",
    "    # Join all parts with spaces\n",
    "    search_text = ' '.join(str(p) for p in parts if p)\n",
    "    \n",
    "    # Clean up multiple spaces\n",
    "    search_text = re.sub(r'\\s+', ' ', search_text).strip()\n",
    "    \n",
    "    return search_text\n",
    "\n",
    "\n",
    "# Build search_text for all entries\n",
    "for entry in data:\n",
    "    entry['search_text'] = build_search_text(entry)\n",
    "\n",
    "# Verify\n",
    "print(\"search_text built for all entries\")\n",
    "print(f\"\\nTotal entries: {len(data)}\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example search_text (headword entry):\")\n",
    "print(\"=\"*80)\n",
    "headword_example = next(e for e in data if e['entry_type'] == 'headword' and e['definitions'])\n",
    "print(f\"Entry: {headword_example['headword_devanagari']}\")\n",
    "print(f\"search_text: {headword_example['search_text'][:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example search_text (entry with referenced_entry):\")\n",
    "print(\"=\"*80)\n",
    "ref_example = next((e for e in data if e['definitions'] and \n",
    "                    e['definitions'][0].get('referenced_entry')), None)\n",
    "if ref_example:\n",
    "    print(f\"Entry: {ref_example['headword_devanagari']}\")\n",
    "    print(f\"search_text: {ref_example['search_text'][:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example search_text (collocation entry):\")\n",
    "print(\"=\"*80)\n",
    "colloc_example = next((e for e in data if e['entry_type'] == 'collocation'), None)\n",
    "if colloc_example:\n",
    "    print(f\"Entry: {colloc_example['headword_devanagari']}\")\n",
    "    print(f\"search_text: {colloc_example['search_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "22721d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL PROCESSING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total entries: 10460\n",
      "  - Headword entries: 9836\n",
      "  - Collocation entries: 624\n",
      "\n",
      "Referenced entries by relationship:\n",
      "  - cross_reference: 81\n",
      "  - base_form: 39\n",
      "  - reduplication_of: 16\n",
      "  - abbreviation_of: 4\n",
      "\n",
      "Entries with search_text: 10460\n",
      "Average search_text length: 69.7 characters\n",
      "\n",
      "Entries with base_word: 624\n",
      "\n",
      "================================================================================\n",
      "SCHEMA VERIFICATION\n",
      "================================================================================\n",
      "✓ All entries have required fields\n",
      "✓ All definitions have required fields\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 11: Summary Statistics\n",
    "# ==============================================================================\n",
    "# This cell:\n",
    "# - Provides final statistics on processed data\n",
    "# - Counts entries by type\n",
    "# - Counts referenced_entry by relationship type\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL PROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Entry type counts\n",
    "headword_count = sum(1 for e in data if e['entry_type'] == 'headword')\n",
    "collocation_count = sum(1 for e in data if e['entry_type'] == 'collocation')\n",
    "\n",
    "print(f\"\\nTotal entries: {len(data)}\")\n",
    "print(f\"  - Headword entries: {headword_count}\")\n",
    "print(f\"  - Collocation entries: {collocation_count}\")\n",
    "\n",
    "# Referenced entry counts by relationship\n",
    "relationship_counts = {}\n",
    "for entry in data:\n",
    "    for defn in entry['definitions']:\n",
    "        ref = defn.get('referenced_entry')\n",
    "        if ref:\n",
    "            rel = ref['relationship']\n",
    "            relationship_counts[rel] = relationship_counts.get(rel, 0) + 1\n",
    "\n",
    "print(f\"\\nReferenced entries by relationship:\")\n",
    "for rel, count in sorted(relationship_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  - {rel}: {count}\")\n",
    "\n",
    "# Entries with search_text\n",
    "has_search_text = sum(1 for e in data if e['search_text'])\n",
    "print(f\"\\nEntries with search_text: {has_search_text}\")\n",
    "\n",
    "# Average search_text length\n",
    "avg_len = sum(len(e['search_text']) for e in data if e['search_text']) / has_search_text\n",
    "print(f\"Average search_text length: {avg_len:.1f} characters\")\n",
    "\n",
    "# Entries with base_word (collocations)\n",
    "has_base_word = sum(1 for e in data if e['base_word'])\n",
    "print(f\"\\nEntries with base_word: {has_base_word}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCHEMA VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verify all entries have required fields\n",
    "required_fields = ['entry_id', 'headword_devanagari', 'headword_romanized', \n",
    "                   'full_entry', 'source_page', 'entry_type', 'definitions',\n",
    "                   'base_word', 'search_text']\n",
    "\n",
    "missing_fields = []\n",
    "for entry in data:\n",
    "    for field in required_fields:\n",
    "        if field not in entry:\n",
    "            missing_fields.append((entry['entry_id'], field))\n",
    "\n",
    "if missing_fields:\n",
    "    print(f\"WARNING: {len(missing_fields)} entries missing required fields\")\n",
    "    for entry_id, field in missing_fields[:10]:\n",
    "        print(f\"  {entry_id} missing {field}\")\n",
    "else:\n",
    "    print(\"✓ All entries have required fields\")\n",
    "\n",
    "# Verify definition schema\n",
    "def_required = ['definition', 'pos_display', 'number', 'pos', 'gender', \n",
    "                'declension_class', 'referenced_entry']\n",
    "\n",
    "def_missing = []\n",
    "for entry in data:\n",
    "    for i, defn in enumerate(entry['definitions']):\n",
    "        for field in def_required:\n",
    "            if field not in defn:\n",
    "                def_missing.append((entry['entry_id'], i, field))\n",
    "\n",
    "if def_missing:\n",
    "    print(f\"WARNING: {len(def_missing)} definitions missing required fields\")\n",
    "    for entry_id, idx, field in def_missing[:10]:\n",
    "        print(f\"  {entry_id} def[{idx}] missing {field}\")\n",
    "else:\n",
    "    print(\"✓ All definitions have required fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "73eea016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 10460 entries to berntsen_dictionary_processed.json\n",
      "Exported 50 sample entries to berntsen_dictionary_sample.json\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 12: Export to JSON\n",
    "# ==============================================================================\n",
    "# This cell:\n",
    "# - Exports the processed data to a JSON file\n",
    "# - Ready for import into vector database\n",
    "\n",
    "import json\n",
    "\n",
    "output_filename = 'berntsen_dictionary_processed.json'\n",
    "\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Exported {len(data)} entries to {output_filename}\")\n",
    "\n",
    "# Also export a sample for inspection\n",
    "sample_filename = 'berntsen_dictionary_sample.json'\n",
    "sample_data = data[:50]  # First 50 entries\n",
    "\n",
    "with open(sample_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Exported {len(sample_data)} sample entries to {sample_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "79e4450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification of All entries with devanagari script\n",
    "\n",
    "#| Category | Count (approx) | Action | `entry_type` | Needs `referenced_entry`? |\n",
    "#|----------|----------------|--------|--------------|---------------------------|\n",
    "#| **A. Cross-references** (\"See X\") | ~50 | Keep as-is, add relationship | `headword` | Yes: `relationship: 'cross_reference'` |\n",
    "#| **B. Collocations** (० patterns) | ~400 | Create NEW entries | `collocation` | No - uses `base_word` instead |\n",
    "#| **C. Grammatical forms** (perf. of, past tense of, etc.) | ~40 | Add referenced_entry | `headword` | Yes: `relationship: 'base_form'` |\n",
    "#| **D. Reduplication** (redupl. of) | ~20 | Add referenced_entry | `headword` | Yes: `relationship: 'reduplication_of'` |\n",
    "#| **E. Abbreviations** (abbrev. of) | ~15 | Add referenced_entry | `headword` | Yes: `relationship: 'abbreviation_of'` |\n",
    "#| **F. Variants** (var. of) | ~5 | Add referenced_entry | `headword` | Yes: `relationship: 'variant_of'` |\n",
    "#| **G. Self-contained definitions** (measurements, cultural terms, suffixes, intensifiers, etc.) | ~400+ | No structural change | `headword` | No - Devanagari is just part of definition text |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfcc748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fcb87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
